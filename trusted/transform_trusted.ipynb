{
	"metadata": {
		"kernelspec": {
			"display_name": "Glue PySpark",
			"language": "python",
			"name": "glue_pyspark"
		},
		"language_info": {
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"file_extension": ".py",
			"mimetype": "text/x-python",
			"name": "Python_Glue_Session",
			"pygments_lexer": "python3"
		}
	},
	"nbformat_minor": 4,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "code",
			"source": "%extra_py_files s3://775307465848-dependencies/dependencies/pydeequ.zip",
			"metadata": {
				"trusted": true,
				"tags": [],
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"execution_count": 2,
			"outputs": [
				{
					"name": "stdout",
					"text": "Welcome to the Glue Interactive Sessions Kernel\nFor more information on available magic commands, please type %help in any new cell.\n\nPlease view our Getting Started page to access the most up-to-date information on the Interactive Sessions kernel: https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions.html\nInstalled kernel version: 1.0.4 \nExtra py files to be included:\ns3://775307465848-dependencies/dependencies/pydeequ.zip\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "%extra_jars s3://775307465848-dependencies/dependencies/deequ-2.0.4-spark-3.3.jar",
			"metadata": {
				"trusted": true,
				"tags": [],
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"execution_count": 4,
			"outputs": [
				{
					"name": "stdout",
					"text": "Extra jars to be included:\ns3://775307465848-dependencies/dependencies/deequ-2.0.4-spark-3.3.jar\ns3://775307465848-dependencies/dependencies/deequ-2.0.4-spark-3.3.jar\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "%idle_timeout 2880\n%glue_version 3.0\n%worker_type G.1X\n%number_of_workers 2\n\nimport sys\nimport os\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\n\nsc = SparkContext.getOrCreate()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)",
			"metadata": {
				"trusted": true,
				"tags": [],
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"execution_count": 1,
			"outputs": [
				{
					"name": "stdout",
					"text": "Current idle_timeout is None minutes.\nidle_timeout has been set to 2880 minutes.\nSetting Glue version to: 3.0\nPrevious worker type: None\nSetting new worker type to: G.1X\nPrevious number of workers: None\nSetting new number of workers to: 2\nTrying to create a Glue session for the kernel.\nSession Type: glueetl\nWorker Type: G.1X\nNumber of Workers: 2\nSession ID: 1911a9b1-d21d-407a-bff8-e18787bc7a79\nApplying the following default arguments:\n--glue_kernel_version 1.0.4\n--enable-glue-datacatalog true\n--extra-py-files s3://775307465848-dependencies/dependencies/pydeequ.zip\n--extra-jars s3://775307465848-dependencies/dependencies/deequ-2.0.4-spark-3.3.jar\nWaiting for session 1911a9b1-d21d-407a-bff8-e18787bc7a79 to get into ready status...\nSession 1911a9b1-d21d-407a-bff8-e18787bc7a79 has been created.\n\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "import datetime",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 2,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "os.environ[\"SPARK_VERSION\"]='3.3'",
			"metadata": {
				"trusted": true,
				"tags": [],
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"execution_count": 3,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "from pydeequ.checks import *\nfrom pydeequ.verification import *",
			"metadata": {
				"trusted": true,
				"tags": [],
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"execution_count": 4,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "import os\n\nclass DataWriter:\n    def write_parquet(self, df, output_directory, partitioned_by, mode=\"overwrite\"):\n        df.write.partitionBy(partitioned_by).mode(mode).parquet(output_directory)",
			"metadata": {
				"trusted": true,
				"tags": [],
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"execution_count": 5,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "from pyspark.sql.types import StructType, StructField, StringType, StringType, DoubleType, LongType, ArrayType\n\nSCHEMA = StructType([\n    StructField(\"test_keys\", StructType([\n        StructField(\"accessible\", StringType(), True),\n        StructField(\"agent\", StringType(), True),\n        StructField(\"blocking\", StringType(), True),\n        StructField(\"body_length_match\", StringType(), True),\n        StructField(\"body_proportion\", DoubleType(), True),\n        StructField(\"client_resolver\", StringType(), True),\n        StructField(\"control\", StructType([\n            StructField(\"dns\", StructType([\n                StructField(\"addrs\", ArrayType(StringType(), True), True),\n                StructField(\"failure\", StringType(), True)\n            ]), True),\n            StructField(\"http_request\", StructType([\n                StructField(\"body_length\", LongType(), True),\n                StructField(\"failure\", StringType(), True),\n                StructField(\"headers\", StructType([\n                    StructField(\"Accept-Ranges\", StringType(), True),\n                    StructField(\"CF-RAY\", StringType(), True),\n                    StructField(\"Cache-Control\", StringType(), True),\n                    StructField(\"Content-Language\", StringType(), True),\n                    StructField(\"Content-Location\", StringType(), True),\n                    StructField(\"Content-Type\", StringType(), True),\n                    StructField(\"Date\", StringType(), True),\n                    StructField(\"ETag\", StringType(), True),\n                    StructField(\"Expires\", StringType(), True),\n                    StructField(\"Last-Modified\", StringType(), True),\n                    StructField(\"Server\", StringType(), True),\n                    StructField(\"Set-Cookie\", StringType(), True),\n                    StructField(\"Strict-Transport-Security\", StringType(), True),\n                    StructField(\"TCN\", StringType(), True),\n                    StructField(\"Vary\", StringType(), True),\n                    StructField(\"X-Content-Type-Options\", StringType(), True),\n                    StructField(\"X-Frame-Options\", StringType(), True),\n                    StructField(\"X-Xss-Protection\", StringType(), True),\n                    StructField(\"content-encoding\", StringType(), True)\n                ]), True),\n                StructField(\"status_code\", LongType(), True),\n                StructField(\"title\", StringType(), True)\n            ]), True)\n        ]), True),\n        StructField(\"control_failure\", StringType(), True),\n        StructField(\"dns_consistency\", StringType(), True),\n        StructField(\"dns_experiment_failure\", StringType(), True),\n        StructField(\"headers_match\", StringType(), True),\n        StructField(\"http_experiment_failure\", StringType(), True),\n        StructField(\"queries\", ArrayType(StructType([\n            StructField(\"answers\", ArrayType(StructType([\n                StructField(\"answer_type\", StringType(), True),\n                StructField(\"ipv4\", StringType(), True)\n            ]), True), True),\n            StructField(\"failure\", StringType(), True),\n            StructField(\"hostname\", StringType(), True),\n            StructField(\"query_type\", StringType(), True),\n            StructField(\"resolver_hostname\", StringType(), True),\n            StructField(\"resolver_port\", StringType(), True)\n        ]), True), True),\n        StructField(\"requests\", ArrayType(StructType([\n            StructField(\"failure\", StringType(), True),\n            StructField(\"request\", StructType([\n                StructField(\"body\", StringType(), True),\n                StructField(\"headers\", StructType([\n                    StructField(\"Accept\", StringType(), True),\n                    StructField(\"Accept-Language\", StringType(), True),\n                    StructField(\"User-Agent\", StringType(), True)\n                ]), True),\n                StructField(\"method\", StringType(), True),\n                StructField(\"tor\", StructType([\n                    StructField(\"exit_ip\", StringType(), True),\n                    StructField(\"exit_name\", StringType(), True),\n                    StructField(\"is_tor\", StringType(), True)\n                ]), True),\n                StructField(\"url\", StringType(), True)\n            ]), True),\n            StructField(\"response\", StructType([\n                StructField(\"body\", StringType(), True),\n                StructField(\"code\", LongType(), True),\n                StructField(\"headers\", StructType([\n                    StructField(\"Accept-Ranges\", StringType(), True),\n                    StructField(\"Cache-Control\", StringType(), True),\n                    StructField(\"Content-Language\", StringType(), True),\n                    StructField(\"Content-Location\", StringType(), True),\n                    StructField(\"Content-Type\", StringType(), True),\n                    StructField(\"Date\", StringType(), True),\n                    StructField(\"ETag\", StringType(), True),\n                    StructField(\"Expires\", StringType(), True),\n                    StructField(\"Last-Modified\", StringType(), True),\n                    StructField(\"Location\", StringType(), True),\n                    StructField(\"Server\", StringType(), True),\n                    StructField(\"Strict-Transport-Security\", StringType(), True),\n                    StructField(\"TCN\", StringType(), True),\n                    StructField(\"Vary\", StringType(), True),\n                    StructField(\"X-Content-Type-Options\", StringType(), True),\n                    StructField(\"X-Frame-Options\", StringType(), True),\n                    StructField(\"X-Xss-Protection\", StringType(), True),\n                    StructField(\"content-encoding\", StringType(), True)\n                ]), True)\n            ]), True)\n        ]), True), True),\n        StructField(\"retries\", LongType(), True),\n        StructField(\"socksproxy\", StringType(), True),\n        StructField(\"status_code_match\", StringType(), True),\n        StructField(\"tcp_connect\", ArrayType(StructType([\n            StructField(\"ip\", StringType(), True),\n            StructField(\"port\", LongType(), True),\n            StructField(\"status\", StructType([\n                StructField(\"blocked\", StringType(), True),\n                StructField(\"failure\", StringType(), True),\n                StructField(\"success\", StringType(), True)\n            ]), True)\n        ]), True), True),\n        StructField(\"title_match\", StringType(), True)\n    ]), True),\n    StructField(\"annotations\", StructType([\n        StructField(\"platform\", StringType(), True)\n    ]), True),\n    StructField(\"backend_version\", StringType(), True),\n    StructField(\"bucket_date\", StringType(), True),\n    StructField(\"data_format_version\", StringType(), True),\n    StructField(\"id\", StringType(), True),\n    StructField(\"input\", StringType(), True),\n    StructField(\"input_hashes\", ArrayType(StringType(), True), True),\n    StructField(\"measurement_start_time\", StringType(), True),\n    StructField(\"options\", ArrayType(StringType(), True), True),\n    StructField(\"probe_asn\", StringType(), True),\n    StructField(\"probe_cc\", StringType(), True),\n    StructField(\"probe_city\", StringType(), True),\n    StructField(\"probe_ip\", StringType(), True),\n    StructField(\"report_filename\", StringType(), True),\n    StructField(\"report_id\", StringType(), True),\n    StructField(\"software_name\", StringType(), True),\n    StructField(\"software_version\", StringType(), True),\n    StructField(\"test_helpers\", StructType([\n        StructField(\"backend\", StructType([\n            StructField(\"address\", StringType(), True),\n            StructField(\"type\", StringType(), True)\n        ]), True)\n    ]), True),\n    StructField(\"test_name\", StringType(), True),\n    StructField(\"test_runtime\", DoubleType(), True),\n    StructField(\"test_start_time\", StringType(), True),\n    StructField(\"test_version\", StringType(), True)\n])",
			"metadata": {
				"trusted": true,
				"tags": [],
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"execution_count": 6,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "from pyspark.sql import DataFrame\n\nclass UnsupportedFileType(Exception):\n    def __init__(self, file_type):\n        self.file_type = file_type\n        self.message = f\"File(s) of type {file_type} not supported\"\n        super().__init__(self.message)\n\nclass ExtractData:\n    def __init__(self, spark, file_directory: list, file_type: str):\n        self.file_directory = file_directory\n        self.file_type = file_type\n        self.spark = spark\n\n    def extract(self) -> DataFrame:\n        if self.file_type in ('json'):\n            return self.spark.read.format(\"json\")\\\n                .option(\"recursiveFileLookup\", \"true\")\\\n                .option(\"pathGlobFilter\",\"*.json\")\\\n            .load(self.file_directory, schema=SCHEMA)\n        else:\n            raise UnsupportedFileType(self.file_type)",
			"metadata": {
				"trusted": true,
				"tags": [],
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"execution_count": 7,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "from abc import ABC, abstractmethod\nimport pyspark.sql.functions as F\nfrom pyspark.sql.types import *\nfrom pyspark.sql import DataFrame\nfrom pydeequ.analyzers import *\nfrom pydeequ.verification import VerificationSuite\nimport os\n\n\nclass TransformData(ABC):\n    \"\"\"\n    Gathers general functions for all transformations.\n    \"\"\"\n    def load_column_rename_mappings(self, transformation_name):\n        column_rename_mappings = json_column_rename\n        return column_rename_mappings.get(transformation_name, {})\n\n\n    def rename_columns(self, df: DataFrame, column_rename) -> DataFrame:\n        for old_name, new_name in column_rename.items():\n            df = df.withColumnRenamed(old_name, new_name)\n        return df\n\n    @abstractmethod\n    def transform(self) -> DataFrame:\n        pass\n    \n    @abstractmethod\n    def data_quality(self):\n        pass\n\n\nclass Transformation(TransformData):\n    \"\"\"\n    Functions for transforming the pandas dataframe for banks.\n    \"\"\"\n    def __init__(self, df: DataFrame, spark):\n        \"\"\"\n        Receives the dataframe.\n        \"\"\"\n        self.df = df\n        self.spark = spark\n\n\n    def transform(self) -> DataFrame:\n\n        transformed_df = self.df \\\n        .select(\n            F.col('id'),\n            F.col('input'),\n            F.col('measurement_start_time'),\n            F.col('test_start_time'),\n            F.col('probe_asn'),\n            F.col('probe_cc'),\n            F.col('probe_ip'),\n            F.col('report_id'),\n            F.col('software_name'),\n            F.col('software_version'),\n            F.col('test_name'),\n            F.col('test_runtime'),\n            F.col('test_version'),\n            F.col('bucket_date'),\n            F.col('test_keys.queries').alias('queries'),\n            F.col('test_keys.control_failure').alias('control_failure'),\n            F.col('test_keys.blocking').alias('blocking'),\n            F.col('test_keys.http_experiment_failure').alias('http_experiment_failure'),\n            F.col('test_keys.tcp_connect').alias('tcp_connect'),\n            F.col('test_keys.requests').alias('requests'),\n            F.col('test_keys.control').alias('control'),\n            F.col('test_keys.dns_experiment_failure').alias('dns_experiment_failure'),\n            F.col('annotations.platform').alias('platform')\n        )\n        \n        transformed_df = transformed_df.withColumn(\"bucket_date\", F.to_date(F.col(\"bucket_date\")))\\\n                                       .withColumn(\"measurement_start_time\", F.to_timestamp(\"measurement_start_time\"))\\\n                                       .withColumn(\"test_start_time\", F.to_timestamp(\"test_start_time\"))\\\n                                       .withColumn(\"row_id\", F.md5(F.concat(F.col(\"id\"), F.col(\"measurement_start_time\"), F.col(\"input\"))))\\\n                                       .withColumn(\"is_ip_valid\", F.when(F.col(\"probe_ip\").rlike(r\"^((25[0-5]|(2[0-4]|1\\d|[1-9]|)\\d)\\.?\\b){4}$\"), F.lit(True)))\\\n                                       .withColumn(\"is_url_valid\", F.when(F.col(\"input\").rlike(r\"^(https?:\\/\\/)?([\\da-z\\.-]+)\\.([a-z\\.]{2,6})([\\/\\w \\.-]*)*\\/?$\"), F.lit(True)))\\\n                                       .dropDuplicates([\"row_id\"])\n\n    \n        return transformed_df\n\n    def data_quality(self, df):\n\n        check = Check(self.spark, CheckLevel.Warning, \"Review Check\")\n        check_result = (VerificationSuite(self.spark)\n                        .onData(df)\n                        .addCheck(check\n                        .isUnique(\"row_id\")\n                        .hasCompleteness(\"id\", lambda completeness: completeness >= 0.9)\n                        .hasCompleteness(\"probe_asn\", lambda completeness: completeness >= 0.9)\n                        .hasCompleteness(\"probe_cc\", lambda completeness: completeness >= 0.98)\n                        .hasCompleteness(\"probe_ip\", lambda completeness: completeness >= 0.9)\n                        .hasCompleteness(\"test_start_time\", lambda completeness: completeness >= 0.9)\n                        .hasCompleteness(\"test_name\", lambda completeness: completeness >= 0.9)\n                        .hasCompleteness(\"bucket_date\", lambda completeness: completeness >= 0.98)\n                        .hasCompleteness(\"measurement_start_time\", lambda completeness: completeness >= 0.9)\n                        .isContainedIn(\"probe_cc\", [\"BR\", \"CN\", \"FR\", \"RU\", \"GB\", \"US\", \"DE\", \"IN\", \"AR\"])\n                        .hasCompleteness(\"is_ip_valid\", lambda completeness: completeness >= 0.9)\n                        .hasCompleteness(\"is_url_valid\", lambda completeness: completeness >= 0.9)\n                        .hasSize(lambda size: size > 0))\n                        .run())\n        \n        df_check_result = VerificationResult.checkResultsAsDataFrame(self.spark,check_result)\n        \n        df_check_result.write.mode(\"append\").parquet('s3://775307465848-logs/validation/trusted/{}'.format(datetime.datetime.now().isoformat()))\n        \n        df_check_result_error = df_check_result.filter(F.col(\"constraint_status\") != \"Success\")\n        if len(df_check_result_error.take(1)) != 0:\n            print(\"ERROR - DATAQUALITY\")\n            subset_drop = []\n            flag_probe_cc = False\n\n            collect_errors = df_check_result_error.collect()\n\n            for row in collect_errors:\n                if \"id\" in row.constraint:\n                    subset_drop.append(\"id\")\n                if \"probe_asn\" in row.constraint:\n                    subset_drop.append(\"probe_asn\")\n                if \"probe_cc\" in row.constraint:\n                    subset_drop.append(\"probe_cc\")\n                if \"probe_ip\" in row.constraint:\n                    subset_drop.append(\"probe_ip\")\n                if \"test_start_time\" in row.constraint:\n                    subset_drop.append(\"test_start_time\")\n                if \"test_name\" in row.constraint:\n                    subset_drop.append(\"test_name\")\n                if \"bucket_date\" in row.constraint:\n                    subset_drop.append(\"bucket_date\")\n                if \"measurement_start_time\" in row.constraint:\n                    subset_drop.append(\"measurement_start_time\")\n                if \"is_ip_valid\" in row.constraint:\n                    subset_drop.append(\"is_ip_valid\")\n                if \"is_url_valid\" in row.constraint:\n                    subset_drop.append(\"is_url_valid\")\n                if \"probe_cc\" in row.constraint:\n                    flag_probe_cc = True\n\n            if subset_drop:\n                df = df.na.drop(subset=subset_drop)\n            if flag_probe_cc:\n                df = df.where(F.col(\"probe_cc\").isin([\"BR\", \"CN\", \"FR\", \"RU\", \"GB\", \"US\", \"DE\", \"IN\", \"AR\"]))\n        \n        return df    \n        ",
			"metadata": {
				"trusted": true,
				"tags": [],
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"execution_count": 46,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "extract_data = ExtractData(\n    spark=spark,\n    file_directory='s3://775307465848-raw/', \n    file_type='json'\n)\n\ndf_raw = extract_data.extract()\n\ntransform_data = Transformation(df_raw, spark)\ndf_ooni = transform_data.transform()\n\ndf_ooni_validated = transform_data.data_quality(df_ooni)\n\noutput_directory = 's3://775307465848-trusted/trusted'\nwrite_data = DataWriter()\nwrite_data.write_parquet(df_ooni_validated, output_directory, ['bucket_date', 'probe_cc'])\n\njob.commit()",
			"metadata": {
				"trusted": true,
				"tags": [],
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"execution_count": 47,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		}
	]
}